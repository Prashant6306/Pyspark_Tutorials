{"cells":[{"source":"<a href=\"https://www.kaggle.com/code/prashantsparhad/22-pyspark-create-empty-rdd-in-pyspark?scriptVersionId=133653031\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","execution_count":1,"id":"cb83b729","metadata":{"execution":{"iopub.execute_input":"2023-06-15T07:45:41.085114Z","iopub.status.busy":"2023-06-15T07:45:41.084742Z","iopub.status.idle":"2023-06-15T07:46:37.143448Z","shell.execute_reply":"2023-06-15T07:46:37.142081Z"},"papermill":{"duration":56.078793,"end_time":"2023-06-15T07:46:37.147337","exception":false,"start_time":"2023-06-15T07:45:41.068544","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting pyspark\r\n","  Downloading pyspark-3.4.0.tar.gz (310.8 MB)\r\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l-\b \b\\\b \b|\b \bdone\r\n","\u001b[?25hRequirement already satisfied: py4j==0.10.9.7 in /opt/conda/lib/python3.10/site-packages (from pyspark) (0.10.9.7)\r\n","Building wheels for collected packages: pyspark\r\n","  Building wheel for pyspark (setup.py) ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \bdone\r\n","\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.4.0-py2.py3-none-any.whl size=311317146 sha256=d80868a0bc1b9ba1569a93c6d5bb088a4dfd94a14b282a719afb58eab7474302\r\n","  Stored in directory: /root/.cache/pip/wheels/7b/1b/4b/3363a1d04368e7ff0d408e57ff57966fcdf00583774e761327\r\n","Successfully built pyspark\r\n","Installing collected packages: pyspark\r\n","Successfully installed pyspark-3.4.0\r\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n","\u001b[0m"]}],"source":["!pip install pyspark"]},{"cell_type":"markdown","id":"fc593c1f","metadata":{"papermill":{"duration":0.02392,"end_time":"2023-06-15T07:46:37.202492","exception":false,"start_time":"2023-06-15T07:46:37.178572","status":"completed"},"tags":[]},"source":["# 1.Create Empty RDD in PySpark"]},{"cell_type":"code","execution_count":2,"id":"85295636","metadata":{"execution":{"iopub.execute_input":"2023-06-15T07:46:37.252064Z","iopub.status.busy":"2023-06-15T07:46:37.251238Z","iopub.status.idle":"2023-06-15T07:46:37.356867Z","shell.execute_reply":"2023-06-15T07:46:37.355725Z"},"papermill":{"duration":0.132208,"end_time":"2023-06-15T07:46:37.359672","exception":false,"start_time":"2023-06-15T07:46:37.227464","status":"completed"},"tags":[]},"outputs":[],"source":["from pyspark.sql import SparkSession"]},{"cell_type":"code","execution_count":3,"id":"72bfcb3b","metadata":{"execution":{"iopub.execute_input":"2023-06-15T07:46:37.406232Z","iopub.status.busy":"2023-06-15T07:46:37.405426Z","iopub.status.idle":"2023-06-15T07:46:43.541422Z","shell.execute_reply":"2023-06-15T07:46:43.540122Z"},"papermill":{"duration":6.162977,"end_time":"2023-06-15T07:46:43.544732","exception":false,"start_time":"2023-06-15T07:46:37.381755","status":"completed"},"tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":["Setting default log level to \"WARN\".\n","To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n","23/06/15 07:46:41 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"]}],"source":["spark=SparkSession.builder.appName(\"Sparkbyexample\").getOrCreate()"]},{"cell_type":"code","execution_count":4,"id":"e3b8af70","metadata":{"execution":{"iopub.execute_input":"2023-06-15T07:46:43.592485Z","iopub.status.busy":"2023-06-15T07:46:43.591625Z","iopub.status.idle":"2023-06-15T07:46:43.617694Z","shell.execute_reply":"2023-06-15T07:46:43.616444Z"},"papermill":{"duration":0.052886,"end_time":"2023-06-15T07:46:43.620739","exception":false,"start_time":"2023-06-15T07:46:43.567853","status":"completed"},"tags":[]},"outputs":[],"source":["#Creates Empty RDD\n","emptyRDD=spark.sparkContext.emptyRDD()"]},{"cell_type":"code","execution_count":5,"id":"e62773db","metadata":{"execution":{"iopub.execute_input":"2023-06-15T07:46:43.667027Z","iopub.status.busy":"2023-06-15T07:46:43.666606Z","iopub.status.idle":"2023-06-15T07:46:43.67446Z","shell.execute_reply":"2023-06-15T07:46:43.673423Z"},"papermill":{"duration":0.033294,"end_time":"2023-06-15T07:46:43.676441","exception":false,"start_time":"2023-06-15T07:46:43.643147","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["EmptyRDD[0] at emptyRDD at NativeMethodAccessorImpl.java:0\n"]}],"source":["print(emptyRDD)"]},{"cell_type":"markdown","id":"4d9a14cd","metadata":{"papermill":{"duration":0.021768,"end_time":"2023-06-15T07:46:43.720532","exception":false,"start_time":"2023-06-15T07:46:43.698764","status":"completed"},"tags":[]},"source":["# Alternatively you can also get empty RDD by using spark.sparkContext.parallelize([])."]},{"cell_type":"code","execution_count":6,"id":"a42aa3d5","metadata":{"execution":{"iopub.execute_input":"2023-06-15T07:46:43.767882Z","iopub.status.busy":"2023-06-15T07:46:43.766997Z","iopub.status.idle":"2023-06-15T07:46:44.145403Z","shell.execute_reply":"2023-06-15T07:46:44.144073Z"},"papermill":{"duration":0.405932,"end_time":"2023-06-15T07:46:44.148412","exception":false,"start_time":"2023-06-15T07:46:43.74248","status":"completed"},"tags":[]},"outputs":[],"source":["rdd2=spark.sparkContext.parallelize([])"]},{"cell_type":"code","execution_count":7,"id":"cb631482","metadata":{"execution":{"iopub.execute_input":"2023-06-15T07:46:44.194777Z","iopub.status.busy":"2023-06-15T07:46:44.194305Z","iopub.status.idle":"2023-06-15T07:46:44.201876Z","shell.execute_reply":"2023-06-15T07:46:44.200481Z"},"papermill":{"duration":0.033558,"end_time":"2023-06-15T07:46:44.204294","exception":false,"start_time":"2023-06-15T07:46:44.170736","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["ParallelCollectionRDD[1] at readRDDFromFile at PythonRDD.scala:287\n"]}],"source":["print(rdd2)"]},{"cell_type":"markdown","id":"b0408bef","metadata":{"papermill":{"duration":0.02146,"end_time":"2023-06-15T07:46:44.247948","exception":false,"start_time":"2023-06-15T07:46:44.226488","status":"completed"},"tags":[]},"source":["# Note: If you try to perform operations on empty RDD you going to get ValueError(\"RDD is empty\")"]},{"cell_type":"markdown","id":"7a13f59d","metadata":{"papermill":{"duration":0.021032,"end_time":"2023-06-15T07:46:44.290759","exception":false,"start_time":"2023-06-15T07:46:44.269727","status":"completed"},"tags":[]},"source":["# 2. Create Empty DataFrame with Schema (StructType)"]},{"cell_type":"code","execution_count":8,"id":"ebac09b5","metadata":{"execution":{"iopub.execute_input":"2023-06-15T07:46:44.336423Z","iopub.status.busy":"2023-06-15T07:46:44.335951Z","iopub.status.idle":"2023-06-15T07:46:44.341594Z","shell.execute_reply":"2023-06-15T07:46:44.340352Z"},"papermill":{"duration":0.031178,"end_time":"2023-06-15T07:46:44.343814","exception":false,"start_time":"2023-06-15T07:46:44.312636","status":"completed"},"tags":[]},"outputs":[],"source":["from pyspark.sql.types import StructType, StructField, StringType"]},{"cell_type":"code","execution_count":9,"id":"e28f617b","metadata":{"execution":{"iopub.execute_input":"2023-06-15T07:46:44.390987Z","iopub.status.busy":"2023-06-15T07:46:44.390369Z","iopub.status.idle":"2023-06-15T07:46:44.395818Z","shell.execute_reply":"2023-06-15T07:46:44.395038Z"},"papermill":{"duration":0.031036,"end_time":"2023-06-15T07:46:44.398006","exception":false,"start_time":"2023-06-15T07:46:44.36697","status":"completed"},"tags":[]},"outputs":[],"source":["schema=StructType(\n","[\n","    StructField(\"Firstname\",StringType(),True),\n","    StructField(\"middlename\",StringType(),True),\n","    StructField(\"lastname\",StringType(),True)\n","])"]},{"cell_type":"markdown","id":"67236498","metadata":{"papermill":{"duration":0.021557,"end_time":"2023-06-15T07:46:44.442123","exception":false,"start_time":"2023-06-15T07:46:44.420566","status":"completed"},"tags":[]},"source":["# Now use the empty RDD created above and pass it to createDataFrame() of SparkSession along with the schema for columnnames & data types."]},{"cell_type":"code","execution_count":10,"id":"a20688bd","metadata":{"execution":{"iopub.execute_input":"2023-06-15T07:46:44.489233Z","iopub.status.busy":"2023-06-15T07:46:44.488598Z","iopub.status.idle":"2023-06-15T07:46:48.161629Z","shell.execute_reply":"2023-06-15T07:46:48.160293Z"},"papermill":{"duration":3.700849,"end_time":"2023-06-15T07:46:48.165478","exception":false,"start_time":"2023-06-15T07:46:44.464629","status":"completed"},"tags":[]},"outputs":[],"source":["df=spark.createDataFrame(emptyRDD,schema)"]},{"cell_type":"code","execution_count":11,"id":"da339bf3","metadata":{"execution":{"iopub.execute_input":"2023-06-15T07:46:48.21412Z","iopub.status.busy":"2023-06-15T07:46:48.213668Z","iopub.status.idle":"2023-06-15T07:46:48.249163Z","shell.execute_reply":"2023-06-15T07:46:48.24811Z"},"papermill":{"duration":0.063094,"end_time":"2023-06-15T07:46:48.252217","exception":false,"start_time":"2023-06-15T07:46:48.189123","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["root\n"," |-- Firstname: string (nullable = true)\n"," |-- middlename: string (nullable = true)\n"," |-- lastname: string (nullable = true)\n","\n"]}],"source":["df.printSchema()"]},{"cell_type":"markdown","id":"3578eded","metadata":{"papermill":{"duration":0.022326,"end_time":"2023-06-15T07:46:48.297","exception":false,"start_time":"2023-06-15T07:46:48.274674","status":"completed"},"tags":[]},"source":["# 3. Convert Empty RDD to DataFrame"]},{"cell_type":"markdown","id":"f3a5f20c","metadata":{"papermill":{"duration":0.02149,"end_time":"2023-06-15T07:46:48.340388","exception":false,"start_time":"2023-06-15T07:46:48.318898","status":"completed"},"tags":[]},"source":["# You can also create empty DataFrame by converting empty RDD to DataFrameusing toDF(). # "]},{"cell_type":"code","execution_count":12,"id":"2072c337","metadata":{"execution":{"iopub.execute_input":"2023-06-15T07:46:48.390881Z","iopub.status.busy":"2023-06-15T07:46:48.390027Z","iopub.status.idle":"2023-06-15T07:46:48.447898Z","shell.execute_reply":"2023-06-15T07:46:48.446999Z"},"papermill":{"duration":0.087929,"end_time":"2023-06-15T07:46:48.450506","exception":false,"start_time":"2023-06-15T07:46:48.362577","status":"completed"},"tags":[]},"outputs":[],"source":["#Convert empty RDD to Dataframe\n","\n","df1=emptyRDD.toDF(schema)"]},{"cell_type":"code","execution_count":13,"id":"9c86ddae","metadata":{"execution":{"iopub.execute_input":"2023-06-15T07:46:48.497164Z","iopub.status.busy":"2023-06-15T07:46:48.496291Z","iopub.status.idle":"2023-06-15T07:46:48.504153Z","shell.execute_reply":"2023-06-15T07:46:48.502619Z"},"papermill":{"duration":0.034698,"end_time":"2023-06-15T07:46:48.506948","exception":false,"start_time":"2023-06-15T07:46:48.47225","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["root\n"," |-- Firstname: string (nullable = true)\n"," |-- middlename: string (nullable = true)\n"," |-- lastname: string (nullable = true)\n","\n"]}],"source":["df1.printSchema()"]},{"cell_type":"markdown","id":"b327ab09","metadata":{"papermill":{"duration":0.021952,"end_time":"2023-06-15T07:46:48.551458","exception":false,"start_time":"2023-06-15T07:46:48.529506","status":"completed"},"tags":[]},"source":["# 4. Create Empty DataFrame with Schema."]},{"cell_type":"code","execution_count":14,"id":"635ab271","metadata":{"execution":{"iopub.execute_input":"2023-06-15T07:46:48.600813Z","iopub.status.busy":"2023-06-15T07:46:48.599919Z","iopub.status.idle":"2023-06-15T07:46:48.665015Z","shell.execute_reply":"2023-06-15T07:46:48.663566Z"},"papermill":{"duration":0.093811,"end_time":"2023-06-15T07:46:48.667777","exception":false,"start_time":"2023-06-15T07:46:48.573966","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["root\n"," |-- Firstname: string (nullable = true)\n"," |-- middlename: string (nullable = true)\n"," |-- lastname: string (nullable = true)\n","\n"]}],"source":["#Create empty DataFrame directly\n","df2=spark.createDataFrame([],schema)\n","df2.printSchema()"]},{"cell_type":"markdown","id":"2de459a4","metadata":{"papermill":{"duration":0.022651,"end_time":"2023-06-15T07:46:48.713785","exception":false,"start_time":"2023-06-15T07:46:48.691134","status":"completed"},"tags":[]},"source":["# 5. Create Empty DataFrame without Schema (no columns)"]},{"cell_type":"markdown","id":"fe041d72","metadata":{"papermill":{"duration":0.021719,"end_time":"2023-06-15T07:46:48.757518","exception":false,"start_time":"2023-06-15T07:46:48.735799","status":"completed"},"tags":[]},"source":["# To create empty DataFrame with out schema (no columns) just create a empty schema and use it while creating PySpark DataFrame."]},{"cell_type":"code","execution_count":15,"id":"d7f84165","metadata":{"execution":{"iopub.execute_input":"2023-06-15T07:46:48.804029Z","iopub.status.busy":"2023-06-15T07:46:48.803548Z","iopub.status.idle":"2023-06-15T07:46:48.854213Z","shell.execute_reply":"2023-06-15T07:46:48.852785Z"},"papermill":{"duration":0.077286,"end_time":"2023-06-15T07:46:48.857066","exception":false,"start_time":"2023-06-15T07:46:48.77978","status":"completed"},"tags":[]},"outputs":[],"source":["df3=spark.createDataFrame(data=[],schema=StructType([]))"]},{"cell_type":"code","execution_count":16,"id":"ad8831cb","metadata":{"execution":{"iopub.execute_input":"2023-06-15T07:46:48.90426Z","iopub.status.busy":"2023-06-15T07:46:48.903793Z","iopub.status.idle":"2023-06-15T07:46:48.911145Z","shell.execute_reply":"2023-06-15T07:46:48.909427Z"},"papermill":{"duration":0.034251,"end_time":"2023-06-15T07:46:48.913585","exception":false,"start_time":"2023-06-15T07:46:48.879334","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["root\n","\n"]}],"source":["df3.printSchema()"]},{"cell_type":"markdown","id":"18da6854","metadata":{"papermill":{"duration":0.021912,"end_time":"2023-06-15T07:46:48.95764","exception":false,"start_time":"2023-06-15T07:46:48.935728","status":"completed"},"tags":[]},"source":["# Convert PySpark DataFrame to Pandas"]},{"cell_type":"markdown","id":"30b35275","metadata":{"papermill":{"duration":0.021948,"end_time":"2023-06-15T07:46:49.001795","exception":false,"start_time":"2023-06-15T07:46:48.979847","status":"completed"},"tags":[]},"source":["# 1.PySpark DataFrame can be converted to Python Pandas DataFrame using a function toPandas()"]},{"cell_type":"markdown","id":"4e4ab33c","metadata":{"papermill":{"duration":0.021759,"end_time":"2023-06-15T07:46:49.045796","exception":false,"start_time":"2023-06-15T07:46:49.024037","status":"completed"},"tags":[]},"source":["# 2.Pandas run operations on a single node whereas PySpark runs on multiple machines. "]},{"cell_type":"markdown","id":"678cd400","metadata":{"papermill":{"duration":0.022067,"end_time":"2023-06-15T07:46:49.089911","exception":false,"start_time":"2023-06-15T07:46:49.067844","status":"completed"},"tags":[]},"source":["# Prepare PySpark DataFrame"]},{"cell_type":"code","execution_count":17,"id":"980cbf4a","metadata":{"execution":{"iopub.execute_input":"2023-06-15T07:46:49.136854Z","iopub.status.busy":"2023-06-15T07:46:49.136358Z","iopub.status.idle":"2023-06-15T07:46:53.359215Z","shell.execute_reply":"2023-06-15T07:46:53.357062Z"},"papermill":{"duration":4.251132,"end_time":"2023-06-15T07:46:53.363217","exception":false,"start_time":"2023-06-15T07:46:49.112085","status":"completed"},"tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":["23/06/15 07:46:49 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"]},{"name":"stdout","output_type":"stream","text":["root\n"," |-- first_name: string (nullable = true)\n"," |-- middle_name: string (nullable = true)\n"," |-- last_name: string (nullable = true)\n"," |-- dob: string (nullable = true)\n"," |-- gender: string (nullable = true)\n"," |-- salary: long (nullable = true)\n","\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["+----------+-----------+---------+-----+------+------+\n","|first_name|middle_name|last_name|dob  |gender|salary|\n","+----------+-----------+---------+-----+------+------+\n","|James     |           |Smith    |36636|M     |60000 |\n","|Michael   |Rose       |         |40288|M     |70000 |\n","|Robert    |           |Williams |42114|      |400000|\n","|Maria     |Anne       |Jones    |39192|F     |500000|\n","|Jen       |Mary       |Brown    |     |F     |0     |\n","+----------+-----------+---------+-----+------+------+\n","\n"]}],"source":["import pyspark\n","from pyspark.sql import SparkSession\n","spark=SparkSession.builder.appName(\"spark\").getOrCreate()\n","data=[\n","    (\"James\",\"\",\"Smith\",\"36636\",\"M\",60000),\n","    (\"Michael\",\"Rose\",\"\",\"40288\",\"M\",70000),\n","    (\"Robert\",\"\",\"Williams\",\"42114\",\"\",400000),\n","    (\"Maria\",\"Anne\",\"Jones\",\"39192\",\"F\",500000),\n","    (\"Jen\",\"Mary\",\"Brown\",\"\",\"F\",0)\n","\n","]\n","columns=[\"first_name\",\"middle_name\",\"last_name\",\"dob\",\"gender\",\"salary\"]\n","pysparkDF=spark.createDataFrame(data=data,schema=columns)\n","pysparkDF.printSchema()\n","pysparkDF.show(truncate=False)"]},{"cell_type":"markdown","id":"20401683","metadata":{"papermill":{"duration":0.0307,"end_time":"2023-06-15T07:46:53.425907","exception":false,"start_time":"2023-06-15T07:46:53.395207","status":"completed"},"tags":[]},"source":["# Convert PySpark Dataframe to Pandas DataFrame"]},{"cell_type":"markdown","id":"f384b1c2","metadata":{"papermill":{"duration":0.023047,"end_time":"2023-06-15T07:46:53.472833","exception":false,"start_time":"2023-06-15T07:46:53.449786","status":"completed"},"tags":[]},"source":["# PySpark DataFrame provides a method toPandas() to convert Python Pandas DataFrame."]},{"cell_type":"code","execution_count":18,"id":"aabe3771","metadata":{"execution":{"iopub.execute_input":"2023-06-15T07:46:53.52108Z","iopub.status.busy":"2023-06-15T07:46:53.520668Z","iopub.status.idle":"2023-06-15T07:46:54.136473Z","shell.execute_reply":"2023-06-15T07:46:54.135531Z"},"papermill":{"duration":0.643348,"end_time":"2023-06-15T07:46:54.139646","exception":false,"start_time":"2023-06-15T07:46:53.496298","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["  first_name middle_name last_name    dob gender  salary\n","0      James                 Smith  36636      M   60000\n","1    Michael        Rose            40288      M   70000\n","2     Robert              Williams  42114         400000\n","3      Maria        Anne     Jones  39192      F  500000\n","4        Jen        Mary     Brown             F       0\n"]}],"source":["pandasDF=pysparkDF.toPandas()\n","print(pandasDF)"]},{"cell_type":"code","execution_count":19,"id":"6eedfa2d","metadata":{"execution":{"iopub.execute_input":"2023-06-15T07:46:54.199771Z","iopub.status.busy":"2023-06-15T07:46:54.199288Z","iopub.status.idle":"2023-06-15T07:46:54.204666Z","shell.execute_reply":"2023-06-15T07:46:54.203496Z"},"papermill":{"duration":0.038697,"end_time":"2023-06-15T07:46:54.206829","exception":false,"start_time":"2023-06-15T07:46:54.168132","status":"completed"},"tags":[]},"outputs":[],"source":["import pandas as pd"]},{"cell_type":"code","execution_count":20,"id":"69cedfac","metadata":{"execution":{"iopub.execute_input":"2023-06-15T07:46:54.255423Z","iopub.status.busy":"2023-06-15T07:46:54.25501Z","iopub.status.idle":"2023-06-15T07:46:54.281773Z","shell.execute_reply":"2023-06-15T07:46:54.28065Z"},"papermill":{"duration":0.05406,"end_time":"2023-06-15T07:46:54.284198","exception":false,"start_time":"2023-06-15T07:46:54.230138","status":"completed"},"tags":[]},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>first_name</th>\n","      <th>middle_name</th>\n","      <th>last_name</th>\n","      <th>dob</th>\n","      <th>gender</th>\n","      <th>salary</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>James</td>\n","      <td></td>\n","      <td>Smith</td>\n","      <td>36636</td>\n","      <td>M</td>\n","      <td>60000</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Michael</td>\n","      <td>Rose</td>\n","      <td></td>\n","      <td>40288</td>\n","      <td>M</td>\n","      <td>70000</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Robert</td>\n","      <td></td>\n","      <td>Williams</td>\n","      <td>42114</td>\n","      <td></td>\n","      <td>400000</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Maria</td>\n","      <td>Anne</td>\n","      <td>Jones</td>\n","      <td>39192</td>\n","      <td>F</td>\n","      <td>500000</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Jen</td>\n","      <td>Mary</td>\n","      <td>Brown</td>\n","      <td></td>\n","      <td>F</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["  first_name middle_name last_name    dob gender  salary\n","0      James                 Smith  36636      M   60000\n","1    Michael        Rose            40288      M   70000\n","2     Robert              Williams  42114         400000\n","3      Maria        Anne     Jones  39192      F  500000\n","4        Jen        Mary     Brown             F       0"]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["pd.DataFrame(pandasDF)"]},{"cell_type":"code","execution_count":null,"id":"fb2bd691","metadata":{"papermill":{"duration":0.023006,"end_time":"2023-06-15T07:46:54.330831","exception":false,"start_time":"2023-06-15T07:46:54.307825","status":"completed"},"tags":[]},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.10"},"papermill":{"default_parameters":{},"duration":88.641765,"end_time":"2023-06-15T07:46:56.974915","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2023-06-15T07:45:28.33315","version":"2.4.0"}},"nbformat":4,"nbformat_minor":5}